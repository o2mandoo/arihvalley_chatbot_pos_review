"""
LLM ê¸°ë°˜ ë¦¬ë·° ë¶„ì„ ëª¨ë“ˆ
"""
import os
import json
import time
import re
import pandas as pd
from typing import Dict, Optional, List
from openai import OpenAI
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from tqdm.auto import tqdm
import logging

logger = logging.getLogger(__name__)


def load_env_from_shell_rc(var_name: str) -> Optional[str]:
    """zsh ì„¤ì • íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì°¾ì•„ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ í™˜ê²½ë³€ìˆ˜ë¡œ ë¡œë“œ"""
    candidates = [
        os.path.expanduser("~/.zshrc"),
        os.path.expanduser("~/.zshenv"),
        os.path.expanduser("~/.zprofile"),
    ]

    pattern = re.compile(rf"^(export\s+)?{re.escape(var_name)}\s*=\s*(.+)$")

    for path in candidates:
        if not os.path.exists(path):
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    stripped = line.strip()
                    if not stripped or stripped.startswith("#"):
                        continue
                    match = pattern.match(stripped)
                    if not match:
                        continue
                    value = match.group(2).strip()
                    if (value.startswith("'") and value.endswith("'")) or (
                        value.startswith('"') and value.endswith('"')
                    ):
                        value = value[1:-1]
                    if value:
                        os.environ[var_name] = value
                        return value
        except OSError:
            continue

    return None

# ë¶„ì„ í”„ë¡¬í”„íŠ¸
ANALYSIS_PROMPT = """
ë‹¹ì‹ ì€ ìŒì‹ì  ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¦¬ë·°ë¥¼ **ë§¤ìš° ì„¸ë°€í•˜ê²Œ** ë¶„ì„í•˜ì„¸ìš”.

**ë¦¬ë·° ë‚´ìš©:**
{review_content}
---

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "ì‹œì„¤": {{
    "ì ìˆ˜": -5ì—ì„œ +5 ì‚¬ì´ ì •ìˆ˜,
    "ê¸ì •í‚¤ì›Œë“œ": ["ë¶„ìœ„ê¸° ì¢‹ìŒ", "ì¸í…Œë¦¬ì–´ í™í•¨"],
    "ë¶€ì •í‚¤ì›Œë“œ": ["ì¢ìŒ", "ì‹œë„ëŸ¬ì›€"],
    "êµ¬ì²´ì ì–¸ê¸‰": "ì‹œì„¤ ê´€ë ¨ ì–¸ê¸‰ ìš”ì•½",
    "ê°ì •ê°•ë„": 0~10 ì‚¬ì´ ì •ìˆ˜
  }},
  "ì„œë¹„ìŠ¤": {{
    "ì ìˆ˜": -5ì—ì„œ +5 ì‚¬ì´ ì •ìˆ˜,
    "ê¸ì •í‚¤ì›Œë“œ": ["ì¹œì ˆí•¨", "ë¹ ë¥¸ ì‘ëŒ€"],
    "ë¶€ì •í‚¤ì›Œë“œ": ["ì›¨ì´íŒ… ê¹€", "ë¶ˆì¹œì ˆ"],
    "êµ¬ì²´ì ì–¸ê¸‰": "ì„œë¹„ìŠ¤ ê´€ë ¨ ì–¸ê¸‰ ìš”ì•½",
    "ê°ì •ê°•ë„": 0~10 ì‚¬ì´ ì •ìˆ˜
  }},
  "ë§›": {{
    "ì ìˆ˜": -5ì—ì„œ +5 ì‚¬ì´ ì •ìˆ˜,
    "ê¸ì •í‚¤ì›Œë“œ": ["ë§›ìˆìŒ", "ì‹ ì„ í•¨"],
    "ë¶€ì •í‚¤ì›Œë“œ": ["ì§œìš”", "ë³„ë¡œ"],
    "êµ¬ì²´ì ì–¸ê¸‰": "ë§› ê´€ë ¨ ì–¸ê¸‰ ìš”ì•½",
    "ê°ì •ê°•ë„": 0~10 ì‚¬ì´ ì •ìˆ˜
  }},
  "ë©”ë‰´í‰ê°€": [
    {{
      "ë©”ë‰´ëª…": "ì›ì¡°ë‹­ì „ê³¨",
      "í‰ê°€": "ê¸ì •",
      "ì„¸ë¶€ë‚´ìš©": "í•´ë‹¹ ë©”ë‰´ í‰ê°€"
    }}
  ],
  "ìˆ¨ì€ë¶ˆë§Œ": "ë¯¸ë¬˜í•œ ë¶ˆë§Œ/ê°œì„ ì . ì—†ìœ¼ë©´ null",
  "ê³ ê°ë‹ˆì¦ˆ": "ê³ ê°ì´ ì›í•˜ëŠ” ê²ƒ",
  "ê°œì„ ì œì•ˆ": "êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ (ì—†ìœ¼ë©´ null)",
  "ì¬ë°©ë¬¸ì˜ë„": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ/ë¶ˆëª…",
  "ì „ì²´ìš”ì•½": "ë¦¬ë·° í•µì‹¬ 1-2ë¬¸ì¥ ìš”ì•½"
}}

**ê·œì¹™:**
1. ì–¸ê¸‰ì´ ì—†ëŠ” í•­ëª©ì€ ì ìˆ˜ 0, í‚¤ì›Œë“œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
2. í‚¤ì›Œë“œëŠ” ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
3. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
4. í•œêµ­ì–´ë¡œ ì‘ë‹µ
"""


class LLMReviewAnalyzer:
    """LLM ê¸°ë°˜ ë¦¬ë·° ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸
        """
        load_dotenv()

        if not os.getenv("OPENAI_API_KEY"):
            load_env_from_shell_rc("OPENAI_API_KEY")
        if not os.getenv("OPENAI_MODEL"):
            load_env_from_shell_rc("OPENAI_MODEL")

        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError(
                "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! "
                "(.env ë˜ëŠ” ~/.zshrcì— ì„¤ì •í•˜ì„¸ìš”)"
            )

        self.model = model or os.getenv('OPENAI_MODEL', 'gpt-5-mini')
        self.client = OpenAI(api_key=self.api_key)

        # ë³‘ë ¬ ì²˜ë¦¬ìš© ë½
        self.save_lock = threading.Lock()

        logger.info(f"LLM ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    def analyze_single_review(self, review_content: str, max_retries: int = 2) -> Optional[Dict]:
        """
        ë‹¨ì¼ ë¦¬ë·° ë¶„ì„

        Args:
            review_content: ë¦¬ë·° í…ìŠ¤íŠ¸
            max_retries: ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not review_content or pd.isna(review_content):
            return None

        prompt = ANALYSIS_PROMPT.format(review_content=review_content)

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ê³ ê° ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )

                result = json.loads(response.choices[0].message.content)
                return result

            except Exception as e:
                logger.warning(f"ë¶„ì„ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return None
                time.sleep(0.5)

        return None

    def _analyze_single_with_index(self, idx: int, review_content: str) -> tuple:
        """ë³‘ë ¬ ì²˜ë¦¬ìš© ë˜í¼"""
        result = self.analyze_single_review(review_content)
        return idx, result

    def analyze_dataframe(
        self,
        df: pd.DataFrame,
        review_column: str = 'review',
        checkpoint_file: Optional[str] = None,
        max_workers: int = 10
    ) -> pd.DataFrame:
        """
        ë°ì´í„°í”„ë ˆì„ì˜ ë¦¬ë·°ë“¤ì„ ë³‘ë ¬ ë¶„ì„

        Args:
            df: ë¦¬ë·°ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„
            review_column: ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼ëª…
            checkpoint_file: ì¤‘ê°„ ì €ì¥ íŒŒì¼ ê²½ë¡œ
            max_workers: ë³‘ë ¬ ì‘ì—… ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        start_idx = 0
        if checkpoint_file and os.path.exists(checkpoint_file):
            logger.info(f"ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_file}")
            result_df = pd.read_csv(checkpoint_file)

            analyzed_count = result_df['ì‹œì„¤_ì ìˆ˜'].notna().sum()
            logger.info(f"ì´ë¯¸ ë¶„ì„ëœ ë¦¬ë·°: {analyzed_count}ê±´")

            if analyzed_count >= len(df):
                logger.info("ëª¨ë“  ë¦¬ë·°ê°€ ì´ë¯¸ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                return result_df

            start_idx = analyzed_count
        else:
            result_df = df.copy()

            # ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
            new_cols = [
                'ì‹œì„¤_ì ìˆ˜', 'ì‹œì„¤_ê¸ì •í‚¤ì›Œë“œ', 'ì‹œì„¤_ë¶€ì •í‚¤ì›Œë“œ', 'ì‹œì„¤_ì–¸ê¸‰', 'ì‹œì„¤_ê°ì •ê°•ë„',
                'ì„œë¹„ìŠ¤_ì ìˆ˜', 'ì„œë¹„ìŠ¤_ê¸ì •í‚¤ì›Œë“œ', 'ì„œë¹„ìŠ¤_ë¶€ì •í‚¤ì›Œë“œ', 'ì„œë¹„ìŠ¤_ì–¸ê¸‰', 'ì„œë¹„ìŠ¤_ê°ì •ê°•ë„',
                'ë§›_ì ìˆ˜', 'ë§›_ê¸ì •í‚¤ì›Œë“œ', 'ë§›_ë¶€ì •í‚¤ì›Œë“œ', 'ë§›_ì–¸ê¸‰', 'ë§›_ê°ì •ê°•ë„',
                'ë©”ë‰´í‰ê°€_JSON', 'ìˆ¨ì€ë¶ˆë§Œ', 'ê³ ê°ë‹ˆì¦ˆ', 'ê°œì„ ì œì•ˆ', 'ì¬ë°©ë¬¸ì˜ë„', 'ì „ì²´ìš”ì•½'
            ]
            for col in new_cols:
                result_df[col] = None

        total = len(df)
        success_count = 0
        fail_count = 0

        logger.info(f"ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {start_idx}ë²ˆì§¸ë¶€í„° {total}ë²ˆì§¸ê¹Œì§€")
        logger.info(f"ë³‘ë ¬ ì‘ì—… ìˆ˜: {max_workers}")
        logger.info(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {(total - start_idx) * 1.5 / max_workers / 60:.1f}ë¶„")

        # ë¶„ì„í•  ë°ì´í„° ì¤€ë¹„
        tasks = [(idx, df.iloc[idx][review_column]) for idx in range(start_idx, total)]

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._analyze_single_with_index, idx, content): idx
                for idx, content in tasks
            }

            with tqdm(total=len(tasks), desc="ğŸ” LLM ë¶„ì„ ì§„í–‰") as pbar:
                for future in as_completed(futures):
                    idx, result = future.result()

                    if result:
                        try:
                            with self.save_lock:
                                self._save_analysis_to_df(result_df, idx, result)
                                success_count += 1
                        except Exception as e:
                            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (idx={idx}): {e}")
                            fail_count += 1
                    else:
                        fail_count += 1

                    # ì¤‘ê°„ ì €ì¥ (100ê±´ë§ˆë‹¤)
                    if checkpoint_file and (success_count + fail_count) % 100 == 0:
                        with self.save_lock:
                            result_df.to_csv(checkpoint_file, index=False)
                            pbar.set_postfix({'ì„±ê³µ': success_count, 'ì‹¤íŒ¨': fail_count, 'ì €ì¥': 'âœ“'})

                    pbar.update(1)

        # ìµœì¢… ì €ì¥
        if checkpoint_file:
            result_df.to_csv(checkpoint_file, index=False)
            logger.info(f"ìµœì¢… ê²°ê³¼ ì €ì¥: {checkpoint_file}")

        logger.info(f"ë¶„ì„ ì™„ë£Œ - ì„±ê³µ: {success_count}ê±´, ì‹¤íŒ¨: {fail_count}ê±´")

        return result_df

    def _save_analysis_to_df(self, df: pd.DataFrame, idx: int, result: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì €ì¥"""
        # ì‹œì„¤
        df.loc[idx, 'ì‹œì„¤_ì ìˆ˜'] = result.get('ì‹œì„¤', {}).get('ì ìˆ˜', 0)
        df.loc[idx, 'ì‹œì„¤_ê¸ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ì‹œì„¤', {}).get('ê¸ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ì‹œì„¤_ë¶€ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ì‹œì„¤', {}).get('ë¶€ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ì‹œì„¤_ì–¸ê¸‰'] = result.get('ì‹œì„¤', {}).get('êµ¬ì²´ì ì–¸ê¸‰', '')
        df.loc[idx, 'ì‹œì„¤_ê°ì •ê°•ë„'] = result.get('ì‹œì„¤', {}).get('ê°ì •ê°•ë„', 0)

        # ì„œë¹„ìŠ¤
        df.loc[idx, 'ì„œë¹„ìŠ¤_ì ìˆ˜'] = result.get('ì„œë¹„ìŠ¤', {}).get('ì ìˆ˜', 0)
        df.loc[idx, 'ì„œë¹„ìŠ¤_ê¸ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ì„œë¹„ìŠ¤', {}).get('ê¸ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ì„œë¹„ìŠ¤_ë¶€ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ì„œë¹„ìŠ¤', {}).get('ë¶€ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ì„œë¹„ìŠ¤_ì–¸ê¸‰'] = result.get('ì„œë¹„ìŠ¤', {}).get('êµ¬ì²´ì ì–¸ê¸‰', '')
        df.loc[idx, 'ì„œë¹„ìŠ¤_ê°ì •ê°•ë„'] = result.get('ì„œë¹„ìŠ¤', {}).get('ê°ì •ê°•ë„', 0)

        # ë§›
        df.loc[idx, 'ë§›_ì ìˆ˜'] = result.get('ë§›', {}).get('ì ìˆ˜', 0)
        df.loc[idx, 'ë§›_ê¸ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ë§›', {}).get('ê¸ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ë§›_ë¶€ì •í‚¤ì›Œë“œ'] = ', '.join(result.get('ë§›', {}).get('ë¶€ì •í‚¤ì›Œë“œ', []))
        df.loc[idx, 'ë§›_ì–¸ê¸‰'] = result.get('ë§›', {}).get('êµ¬ì²´ì ì–¸ê¸‰', '')
        df.loc[idx, 'ë§›_ê°ì •ê°•ë„'] = result.get('ë§›', {}).get('ê°ì •ê°•ë„', 0)

        # ì¶”ê°€ ë¶„ì„
        df.loc[idx, 'ë©”ë‰´í‰ê°€_JSON'] = json.dumps(result.get('ë©”ë‰´í‰ê°€', []), ensure_ascii=False)
        df.loc[idx, 'ìˆ¨ì€ë¶ˆë§Œ'] = result.get('ìˆ¨ì€ë¶ˆë§Œ', '')
        df.loc[idx, 'ê³ ê°ë‹ˆì¦ˆ'] = result.get('ê³ ê°ë‹ˆì¦ˆ', '')
        df.loc[idx, 'ê°œì„ ì œì•ˆ'] = result.get('ê°œì„ ì œì•ˆ', '')
        df.loc[idx, 'ì¬ë°©ë¬¸ì˜ë„'] = result.get('ì¬ë°©ë¬¸ì˜ë„', 'ë¶ˆëª…')
        df.loc[idx, 'ì „ì²´ìš”ì•½'] = result.get('ì „ì²´ìš”ì•½', '')
