#!/usr/bin/env python3
"""
ì „ì²´ ë¦¬ë·° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
í¬ë¡¤ë§ â†’ ë°ì´í„° ê´€ë¦¬ â†’ LLM ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±
"""
import sys
from pathlib import Path
import logging
import argparse

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from modules import NaverReviewCrawler, ReviewDataManager, LLMReviewAnalyzer, ReportGenerator
from config.config import STORES, CRAWLING_CONFIG, get_review_filepath, get_analysis_filepath

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_full_pipeline(
    store_key: str,
    skip_crawling: bool = False,
    skip_analysis: bool = False,
    max_workers: int = 10
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        store_key: ë§¤ì¥ í‚¤ (ì˜ˆ: "ê°•ë‚¨ì ")
        skip_crawling: í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°
        skip_analysis: ë¶„ì„ ê±´ë„ˆë›°ê¸°
        max_workers: LLM ë¶„ì„ ë³‘ë ¬ ì‘ì—… ìˆ˜
    """
    if store_key not in STORES:
        logger.error(f"ë§¤ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {store_key}")
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ì¥: {list(STORES.keys())}")
        return

    store_info = STORES[store_key]
    store_name = store_info["store_name"]

    logger.info("=" * 100)
    logger.info("ğŸš€ ë¦¬ë·° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("=" * 100)
    logger.info(f"ë§¤ì¥: {store_key} ({store_name})")
    logger.info(f"í¬ë¡¤ë§: {'ê±´ë„ˆë›°ê¸°' if skip_crawling else 'ì‹¤í–‰'}")
    logger.info(f"ë¶„ì„: {'ê±´ë„ˆë›°ê¸°' if skip_analysis else 'ì‹¤í–‰'}")
    logger.info("=" * 100)

    # íŒŒì¼ ê²½ë¡œ
    review_filepath = get_review_filepath(store_name)
    analysis_filepath = get_analysis_filepath(store_name)

    # ========================================
    # 1ë‹¨ê³„: í¬ë¡¤ë§
    # ========================================
    if not skip_crawling:
        logger.info("\n" + "=" * 100)
        logger.info("ğŸ“¡ 1ë‹¨ê³„: ë¦¬ë·° í¬ë¡¤ë§")
        logger.info("=" * 100)

        url = store_info["url"]
        crawler = NaverReviewCrawler(headless=CRAWLING_CONFIG["headless"])

        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            new_reviews = crawler.crawl_reviews(
                url=url,
                wait_time=CRAWLING_CONFIG["wait_time"],
                click_wait=CRAWLING_CONFIG["click_wait"]
            )

            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(new_reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘")

            # ========================================
            # 2ë‹¨ê³„: ë°ì´í„° ê´€ë¦¬ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
            # ========================================
            logger.info("\n" + "=" * 100)
            logger.info("ğŸ’¾ 2ë‹¨ê³„: ë°ì´í„° ê´€ë¦¬ (ì¦ë¶„ ì—…ë°ì´íŠ¸)")
            logger.info("=" * 100)

            data_manager = ReviewDataManager(str(review_filepath))
            data_manager.load_data()

            # ê¸°ì¡´ ë°ì´í„° í†µê³„
            old_stats = data_manager.get_statistics()
            logger.info(f"ê¸°ì¡´ ë¦¬ë·°: {old_stats['total_reviews']}ê±´")

            # ìƒˆë¡œìš´ ë¦¬ë·° ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            added_count = data_manager.merge_and_update(new_reviews)

            # ìƒˆë¡œìš´ í†µê³„
            new_stats = data_manager.get_statistics()
            logger.info(f"ìƒˆë¡œ ì¶”ê°€ëœ ë¦¬ë·°: {added_count}ê±´")
            logger.info(f"ì´ ë¦¬ë·°: {new_stats['total_reviews']}ê±´")

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return
    else:
        logger.info("\ní¬ë¡¤ë§ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    # ========================================
    # 3ë‹¨ê³„: LLM ë¶„ì„
    # ========================================
    if not skip_analysis:
        logger.info("\n" + "=" * 100)
        logger.info("ğŸ¤– 3ë‹¨ê³„: LLM ê¸°ë°˜ ë¦¬ë·° ë¶„ì„")
        logger.info("=" * 100)

        try:
            # ë¦¬ë·° ë°ì´í„° ë¡œë“œ
            import pandas as pd
            review_df = pd.read_csv(review_filepath)
            logger.info(f"ë¶„ì„ ëŒ€ìƒ ë¦¬ë·°: {len(review_df)}ê±´")

            # LLM ë¶„ì„ê¸° ì´ˆê¸°í™”
            analyzer = LLMReviewAnalyzer()

            # ë¶„ì„ ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ ì§€ì›)
            analyzed_df = analyzer.analyze_dataframe(
                df=review_df,
                review_column='review',
                checkpoint_file=str(analysis_filepath),
                max_workers=max_workers
            )

            logger.info(f"ë¶„ì„ ì™„ë£Œ: {str(analysis_filepath)}")

        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return
    else:
        logger.info("\në¶„ì„ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    # ========================================
    # 4ë‹¨ê³„: ë¦¬í¬íŠ¸ ìƒì„±
    # ========================================
    logger.info("\n" + "=" * 100)
    logger.info("ğŸ“Š 4ë‹¨ê³„: ë¦¬í¬íŠ¸ ìƒì„±")
    logger.info("=" * 100)

    try:
        import pandas as pd
        analyzed_df = pd.read_csv(analysis_filepath)

        # ë¶„ì„ ì™„ë£Œëœ ë°ì´í„°ë§Œ í•„í„°ë§
        analyzed_df = analyzed_df[analyzed_df['ì‹œì„¤_ì ìˆ˜'].notna()]
        logger.info(f"ë¶„ì„ ì™„ë£Œëœ ë¦¬ë·°: {len(analyzed_df)}ê±´")

        # ë¦¬í¬íŠ¸ ìƒì„±
        report_gen = ReportGenerator(analyzed_df)
        report = report_gen.generate_full_report()

        # ì½˜ì†” ì¶œë ¥
        print("\n" + report)

        # íŒŒì¼ ì €ì¥
        report_filepath = analysis_filepath.parent / f"{store_name}_report.txt"
        report_gen.save_report(str(report_filepath))

        logger.info(f"ë¦¬í¬íŠ¸ ì €ì¥: {report_filepath}")

    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return

    # ========================================
    # ì™„ë£Œ
    # ========================================
    logger.info("\n" + "=" * 100)
    logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info("=" * 100)
    logger.info(f"ë¦¬ë·° ë°ì´í„°: {review_filepath}")
    logger.info(f"ë¶„ì„ ê²°ê³¼: {analysis_filepath}")
    logger.info(f"ë¦¬í¬íŠ¸: {report_filepath}")
    logger.info("=" * 100)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¦¬ë·° ë¶„ì„ íŒŒì´í”„ë¼ì¸")

    parser.add_argument(
        "--store",
        type=str,
        default=None,
        help=f"ë¶„ì„í•  ë§¤ì¥ (ì˜ˆ: {list(STORES.keys())[0]})"
    )
    parser.add_argument(
        "--skip-crawling",
        action="store_true",
        help="í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)"
    )
    parser.add_argument(
        "--skip-analysis",
        action="store_true",
        help="LLM ë¶„ì„ ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ë¶„ì„ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=10,
        help="LLM ë¶„ì„ ë³‘ë ¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸: 10)"
    )

    args = parser.parse_args()

    # ë§¤ì¥ ì„ íƒ
    if args.store:
        store_key = args.store
    else:
        store_key = list(STORES.keys())[0]
        logger.info(f"ë§¤ì¥ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë§¤ì¥({store_key})ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_full_pipeline(
        store_key=store_key,
        skip_crawling=args.skip_crawling,
        skip_analysis=args.skip_analysis,
        max_workers=args.workers
    )


if __name__ == "__main__":
    main()
